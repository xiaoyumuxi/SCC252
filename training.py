import joblib
import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize
from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc
# è®¾ç½®æ–‡ä»¶è·¯å¾„
file_path = './data/Wednesday-workingHours.pcap_ISCX.csv'

print("æ­£åœ¨è¯»å–æ•°æ®...")

# 1. è¯»å– CSV æ–‡ä»¶
try:
    df = pd.read_csv(file_path)
    print(f"æ•°æ®è¯»å–æˆåŠŸï¼ŒåŸå§‹å½¢çŠ¶: {df.shape}")
except FileNotFoundError:
    print("é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ã€‚")
    exit()

# --- æ•°æ®æ¸…ç†é¢„å¤‡æ­¥éª¤ ---
df.columns = df.columns.str.strip()

# 2. æ¸…ç†æ•°æ® (åˆ é™¤åŒ…å« NaN æˆ– Infinity çš„è¡Œ)
print("æ­£åœ¨æ¸…ç†æ•°æ®...")

# å°†æ— ç©·å¤§ (inf) å’Œè´Ÿæ— ç©·å¤§ (-inf) æ›¿æ¢ä¸º NaN (ç©ºå€¼)
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# åˆ é™¤åŒ…å« NaN çš„è¡Œ
df.dropna(inplace=True)

print(f"æ¸…ç†åå½¢çŠ¶: {df.shape}")

# --- å¤šåˆ†ç±»æ ‡ç­¾ç¼–ç  ---
print("æ­£åœ¨è¿›è¡Œå¤šåˆ†ç±»æ ‡ç­¾ç¼–ç ...")

# æŸ¥çœ‹ä¸€ä¸‹åŸå§‹çš„æ ‡ç­¾éƒ½æœ‰å“ªäº›
print("åŸå§‹æ ‡ç­¾ç±»åˆ«:", df['Label'].unique())

# 3. ä½¿ç”¨ LabelEncoder å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸º 0, 1, 2, 3...
le = LabelEncoder()
df['Label'] = le.fit_transform(df['Label'])

# ä¿å­˜æ˜ å°„å…³ç³»
label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("\næ ‡ç­¾æ˜ å°„å…³ç³»:")
for label, num in label_mapping.items():
    print(f"  {label} -> {num}")

# æŸ¥çœ‹ç¼–ç åçš„åˆ†å¸ƒ
print("\nç¼–ç åçš„æ ‡ç­¾åˆ†å¸ƒ:")
print(df['Label'].value_counts())

# 4. åˆ†ç¦»ç‰¹å¾ (X) å’Œ æ ‡ç­¾ (y)
y = df['Label']
X = df.drop('Label', axis=1)

FEATURE_COLUMNS = X.columns.tolist()

# 5. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
print("\næ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (Stratified)...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # ä¿è¯åˆ‡åˆ†åçš„ç±»åˆ«æ¯”ä¾‹ä¸åŸå§‹æ•°æ®ä¸€è‡´
)

# 6. ç‰¹å¾ç¼©æ”¾ (StandardScaler)
print("æ­£åœ¨è¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–...")
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("\nå¤šåˆ†ç±»é¢„å¤„ç†å®Œæˆï¼")
print("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆã€‚")

# --- 5. æ¨¡å‹è®­ç»ƒ ---
print("\n--- æ­¥éª¤ 5: è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨ (Random Forest) ---")

# å®ä¾‹åŒ–æ¨¡å‹-éšæœºæ£®æ— - ä½¿ç”¨ RandomForestClassifier
# n_estimators=100 è¡¨ç¤ºä½¿ç”¨ 100 æ£µå†³ç­–æ ‘
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)

# åœ¨ç¼©æ”¾åçš„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒ
rf_model.fit(X_train, y_train)

print("æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

# --- 6. æ¨¡å‹è¯„ä¼° ---
print("\n--- æ­¥éª¤ 6: æ¨¡å‹è¯„ä¼° ---")

# ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
# ä½¿ç”¨ weighted average ä»¥é€‚åº”å¤šåˆ†ç±»å’Œå¯èƒ½çš„ä¸å¹³è¡¡æ•°æ®
precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
# å°† 0 è§†ä¸ºè´Ÿç±» (Negative)ï¼Œ1 è§†ä¸ºæ­£ç±» (Positive)
# TNR (ç‰¹å¼‚åº¦) æ˜¯æ¨¡å‹æ­£ç¡®é¢„æµ‹è´Ÿç±»çš„èƒ½åŠ›ï¼Œç›¸å½“äºè´Ÿç±»çš„å¬å›ç‡
recalls = recall_score(y_test, y_pred, average=None, zero_division=0)
# 2. ç±»åˆ« 0 çš„å¬å›ç‡å³ä¸ºç‰¹å¼‚åº¦ (TNR)
# ç±»åˆ« 0 å¯¹åº”æ­£å¸¸æµé‡ ('BENIGN')ï¼Œåœ¨è¿™é‡Œè¢«è®¤å®šä¸ºæ˜¯å‡ç±»
tnr_score = recalls[0]
FPR = 1.0 - tnr_score
# 1. è·å–æ¨¡å‹å¯¹æµ‹è¯•é›†çš„æ¦‚ç‡è¾“å‡º (AUC å¿…éœ€)
y_scores = rf_model.predict_proba(X_test)

# 2. å¯¹çœŸå®æ ‡ç­¾è¿›è¡ŒäºŒå€¼åŒ– (One-Hot ç¼–ç ) ä»¥é€‚åº” OvR ç­–ç•¥
classes = np.unique(y_test)
y_test_binarized = label_binarize(y_test, classes=classes)

# 3. è·å–æ¯ä¸ªç±»åˆ«çš„æ”¯æŒåº¦ (æ ·æœ¬æ•°), ç”¨äºè®¡ç®—åŠ æƒå¹³å‡
support = y_test.value_counts().sort_index().values
total_support = np.sum(support)

roc_auc = dict()
weighted_auc_sum = 0

for i in range(len(classes)):
    # OvR ç­–ç•¥ï¼šè®¡ç®—æ¯ä¸ªç±»åˆ«çš„ AUC
    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_scores[:, i])
    roc_auc[i] = auc(fpr, tpr)

    # è®¡ç®—åŠ æƒå’Œ
    weight = support[i] / total_support
    weighted_auc_sum += roc_auc[i] * weight

auc_weighted = weighted_auc_sum


# æ‰“å°å¯¹åº”çš„æ•°æ®é›†åˆ
print(f"æµ‹è¯•é›†æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"æµ‹è¯•é›†æ•´ä½“ç²¾ç¡®åº¦: {precision:.4f}")
print("---------------------------------")
print(f"æµ‹è¯•é›†[BENIGN]å¬å›ç‡: {recall:.4f}")
print(f"æµ‹è¯•é›†å‡é˜³æ€§ç‡: {FPR:.4f}")
print(f"æµ‹è¯•é›†AUC: {auc_weighted:.4f}")

# å­˜å…¥æ€§èƒ½æŒ‡æ ‡æ˜¾ç¤º
performance_metrics = {
    "accuracy": f"{accuracy:.4f}",
    "precision": f"{precision:.4f}",
    "recall": f"{recall:.4f}",
    "FPR": f"{FPR:.4f}",
    "auc": f"{auc_weighted:.4f}"
}

metrics_filename = './models/ddos_performance.json'
with open(metrics_filename, 'w') as f:
    json.dump(performance_metrics, f)

print(f"- æ€§èƒ½æŒ‡æ ‡: {metrics_filename}")

# æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
print("\n--- å¤šåˆ†ç±»è¯¦ç»†è¯„ä¼°æŠ¥å‘Š ---")
print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))

# --- 7. æ¨¡å‹å’Œé¢„å¤„ç†å™¨ä¿å­˜ ---
print("\n--- æ­¥éª¤ 7: æ¨¡å‹å’Œé¢„å¤„ç†å™¨ä¿å­˜ ---")

model_filename = './models/ddos_rf_model.joblib'
scaler_filename = './models/ddos_scaler.joblib'
encoder_filename = './models/ddos_label_encoder.joblib'
feature_col = './models/ddos_feature_columns.joblib'

# ä½¿ç”¨ joblib ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ (rf_model) å’Œé¢„å¤„ç†å™¨
joblib.dump(rf_model, model_filename)
joblib.dump(scaler, scaler_filename)
joblib.dump(le, encoder_filename)
joblib.dump(FEATURE_COLUMNS, feature_col)

print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
print(f"æ¨¡å‹å’Œé¢„å¤„ç†å™¨å·²ä¿å­˜ä¸º:\n- æ¨¡å‹: {model_filename} (å†…å®¹ä¸ºéšæœºæ£®æ—)\n- ç¼©æ”¾å™¨: {scaler_filename}\n- ç¼–ç å™¨: {encoder_filename}\n- ç‰¹å¾è¡Œåˆ—: {feature_col}")